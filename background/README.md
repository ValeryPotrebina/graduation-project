1. Подготовить и загрузить данные для обучения
2. обучить модель
3. оценить результаты
4. Используйте свою дообученную модель.

# Цены

GPT-4o mini
Fine-tuning price
Input:
$0.30 / 1M tokens
Cached input:
$0.15 / 1M tokens
Output:
$1.20 / 1M tokens
Training:
$3.00 / 1M tokens

Мы рекомендуем сначала попробовать получить хорошие результаты с помощью настройки запросов, цепочек запросов (разбиение сложных задач на несколько запросов) и вызова функций
https://platform.openai.com/docs/guides/function-calling

дообучение эффективно, — это снижение затрат и/или задержки, заменив более дорогую модель, такую как gpt-4o, на дообученную модель gpt-4o-mini. Если вам удается достичь хороших результатов с gpt-4o, вы часто можете получить похожее качество с дообученной моделью gpt-4o-mini, обучив ее на завершениях от gpt-4o, возможно, с укороченной инструкцией.

Подготовка вашего набора данных
Как только вы определили, что дообучение — это правильное решение (например, вы оптимизировали запросы, но модель все еще не решает некоторые проблемы), вам нужно будет подготовить данные для обучения модели. Вы должны создать разнообразный набор демонстрационных диалогов, которые похожи на те, с которыми модель будет работать на этапе использования в реальных приложениях.


# Подготовка вашего набора данных
Каждый пример в наборе данных должен представлять собой диалог в том формате, который используется в нашем API Chat Completions, а именно список сообщений, где каждое сообщение имеет роль, содержание и необязательное имя. По крайней мере, некоторые примеры обучения должны напрямую таргетировать случаи, когда исходная модель не ведет себя как ожидается, а предоставленные ответы от ассистента в данных должны быть идеальными.

Рекомендуем начать с 50 хорошо подобранных примеров и посмотреть, покажет ли модель улучшения после дообучения. В некоторых случаях этого может быть достаточно, но даже если модель еще не готова для использования в продакшн, явные улучшения — хороший сигнал, что предоставление большего объема данных продолжит улучшать модель. Если улучшений нет, возможно, вам нужно пересмотреть, как настроить задачу для модели или переформатировать данные перед масштабированием набора примеров.

Разделение данных на обучающую и тестовую выборки
После сбора начального набора данных мы рекомендуем разделить его на обучающую и тестовую выборки. При отправке задания на дообучение с обеими выборками (обучающей и тестовой) мы будем предоставлять статистику для обеих в процессе обучения. Эти статистики станут вашим начальным сигналом того, как модель улучшает свои результаты. Кроме того, создание тестового набора данных с самого начала будет полезным для оценки модели после обучения, генерируя примеры на тестовом наборе.

Лимиты токенов
Лимиты токенов зависят от выбранной модели. Вот обзор максимальной длины контекста для вывода и контекста примеров для обучения для моделей gpt-4o-mini и gpt-3.5-turbo:

Модель	                    Длина контекста для вывода	    Длина контекста для примеров обучения
gpt-4o-mini-2024-07-18	    128,000 токенов	                65,536 токенов (128k скоро)

1. Выбрать курс и подготовить дата сет (только хранящие текста)
2. Как должен выглядеть дата сет 
3. Разобраться с апи OpenAI
<!-- Задачи -->
Архитектура для бэка
PostMan
Curl
0. Описать подробно че должно быть в проекте
1. Архитектура БД (для авторизации (права и роли) и курсов) и бэка для проекта 
нарисовать реляционку (таблы)

2. 
